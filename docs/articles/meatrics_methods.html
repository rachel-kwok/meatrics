<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="meatrics">
<title>Technical • meatrics</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Technical">
<meta property="og:description" content="meatrics">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">meatrics</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/data_documentation.html">Data</a>
    <a class="dropdown-item" href="../articles/meatrics_methods.html">Technical</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Technical</h1>
            
      
      
      <div class="d-none name"><code>meatrics_methods.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="fleiss-kappa">Fleiss Kappa<a class="anchor" aria-label="anchor" href="#fleiss-kappa"></a>
</h2>
<p><strong>Note</strong>: an assumption of Fleiss Kappa may be violated
in this particular case-study. Fleiss et al. (2003) states: <em>“The
raters responsible for rating one subject are not assumed to be the same
as those responsible for rating another”</em>. The dataset does not
satisfy the assumption that the raters are randomly-selected from a
larger population of raters.</p>
<p>Fleiss Kappa is an extension of Cohen’s Kappa, which assesses
inter-rater reliability for three or more raters for categorical
variables while factoring out agreement due to chance.</p>
<p><span class="math display">\[
\kappa = \frac{\bar{P}-\bar{P}_e}{1-\bar{P}_e}
\]</span></p>
<p><span class="math inline">\(\bar{P}\)</span> is defined as the
observed agreement, <span class="math inline">\(\bar{P}_e\)</span> is
the expected agreement if the ratings were completely random. Hence, the
numerator is defined as the degree of agreement actually achieved above
chance whereas the denominator is the degree of agreement that is
attainable above chance.</p>
<p><code>fleiss_meat()</code> reports on category-wise Fleiss Kappa,
which can be interpreted as the probability of a randomly chosen rater
assigning an item to a category, given another randomly chosen rater has
also assigned that item to the category. The formula for <span class="math inline">\(A_{k}\)</span> , agreement in category <span class="math inline">\(k\)</span> is:</p>
<p><span class="math display">\[
A_{k} =
\frac{\sum_{i=1}^{n}r_{ik}(r_{ik}-1)}{\sum_{i=1}^{n}r_{ik}(r_{i}-1)}
\]</span></p>
<p><span class="math inline">\(r_{ik}\)</span> is the number of graders
who assigned item <span class="math inline">\(i\)</span> to category
<span class="math inline">\(k\)</span> , <span class="math inline">\(r_{i}\)</span> being the total number of ratings
total and <span class="math inline">\(n\)</span> being the number of
items (assuming that each item has been rated by 2 or more graders).</p>
<p>In the context of our dataset, <code>expert_graders</code>, each item
<span class="math inline">\(i\)</span> would be a <code>body_no</code>
and <code>kill_date</code> combination and <span class="math inline">\(k\)</span> would be the available levels for that
categorical/discrete trait.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/fleiss.html">fleiss</a></span><span class="op">(</span><span class="va">expert_graders</span>, device_status <span class="op">=</span> <span class="cn">FALSE</span>, variable <span class="op">=</span> <span class="st">"meat_colour"</span><span class="op">)</span></span></code></pre></div>
<p><img src="meatrics_methods_files/figure-html/unnamed-chunk-1-1.png" width="700"></p>
<p>This function has been extended to allow comparisons between device
category-wise agreement and expert grader category–wise agreement.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/fleiss.html">fleiss</a></span><span class="op">(</span><span class="va">expert_graders</span>, df2 <span class="op">=</span> <span class="va">devices</span>, device_status <span class="op">=</span> <span class="cn">FALSE</span>, variable <span class="op">=</span> <span class="st">"meat_colour"</span><span class="op">)</span></span></code></pre></div>
<p><img src="meatrics_methods_files/figure-html/unnamed-chunk-2-1.png" width="700"></p>
</div>
<div class="section level2">
<h2 id="icc-intra-class-correlation-coefficient">ICC (Intra-class Correlation Coefficient)<a class="anchor" aria-label="anchor" href="#icc-intra-class-correlation-coefficient"></a>
</h2>
<p>The ICC is commonly used to describe how strongly units of the same
group resemble one another. In that sense, it can also be seen as an
assessment of rater agreement. To summarise, ICC can be considered as a
ratio looking at the variance of interest divided by total variance. It
is derived from</p>
<p>There are 10 different types of ICC and 6 defined by Shrout and
Fleiss (1979). It is most relevant to consider the two-way mixed average
score ICC (ICC3k) and the two-way mixed single-rater score (ICC3) for
the purpose of this R package.</p>
<p>This is because the two-way mixed model defines there to be <span class="math inline">\(k\)</span> fixed raters, in which each subject is
measured by the <span class="math inline">\(k\)</span> raters. The
choice of reporting which specific type of ICC statistic is based upon
the future intended usage of the rater. As it is unclear whether each
rater will be rating by itself or if the average of all <span class="math inline">\(k\)</span> raters will be taken as the final
rating, meatrics reports on both ICC3 and ICC3k.</p>
<p>The formula for ICC(3, k) is:</p>
<p><span class="math display">\[
\frac{MS_{R}-MS_{E}}{MS_{R}}
\]</span></p>
<p>In comparison, the formula for ICC(3, 1) is:</p>
<p><span class="math display">\[
\frac{MS_{R}-MS_{E}}{MS_{R} + (k-1)MS_{E}}
\]</span></p>
<p>It can be noted that in the single-rater formula, it is adjusted for
the fact that there is a common source of variability amongst each rater
(<span class="math inline">\(MS_{E}\)</span>).</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># calculating icc across time of devices &amp; expert graders</span></span>
<span><span class="fu"><a href="../reference/icc_line.html">icc_line</a></span><span class="op">(</span><span class="va">expert_graders</span>, df2 <span class="op">=</span> <span class="va">devices</span>, variable <span class="op">=</span> <span class="st">"msa_marbling"</span><span class="op">)</span></span></code></pre></div>
<p><img src="meatrics_methods_files/figure-html/unnamed-chunk-3-1.png" width="700"></p>
</div>
<div class="section level2">
<h2 id="ccc-concordance-correlation-coefficient">CCC (Concordance Correlation Coefficient)<a class="anchor" aria-label="anchor" href="#ccc-concordance-correlation-coefficient"></a>
</h2>
<p>The Concordance Correlation Coefficient is another measure of
inter-rater reliability. It combines measures of precision and accuracy
to determine how far the data deviates from the perfect 45 degree line
of concordance. Its formula is as follows and ranges from -1 to 1, much
like Pearson’s correlation coefficient:</p>
<p><span class="math display">\[
\rho_c =
\frac{2\rho\sigma_x\sigma_y}{\sigma_x^2+\sigma_y^2+(\mu_x-\mu_y)^2}
\]</span></p>
<p><span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> represent the two graders of interest.
The bias correction factor in the formula adjusts for bias in the
measurements by subtracting the difference between the means of the two
sets of measurements from the denominator. This ensures that the CCC is
a more appropriate measure than Pearson’s when there is a systematic
difference between the two graders.</p>
<p><strong><em>References</em></strong>:</p>
<ul>
<li><p><a href="https://github.com/jmgirard/mReliability/wiki/Specific-agreement-coefficient" class="external-link uri">https://github.com/jmgirard/mReliability/wiki/Specific-agreement-coefficient</a></p></li>
<li><p><a href="https://statistics.laerd.com/spss-tutorials/fleiss-kappa-in-spss-statistics.php" class="external-link uri">https://statistics.laerd.com/spss-tutorials/fleiss-kappa-in-spss-statistics.php</a></p></li>
<li><p><a href="https://stats.stackexchange.com/questions/301508/does-cronbachs-alpha-reveal-reliability-information-over-and-above-the-intracla" class="external-link uri">https://stats.stackexchange.com/questions/301508/does-cronbachs-alpha-reveal-reliability-information-over-and-above-the-intracla</a></p></li>
<li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4913118/" class="external-link uri">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4913118/</a></p></li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by The package maintainer.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
